> 本文为中文翻译版本。英文原版为权威版本，如有差异以英文版为准。
> [English Version](../05-risk-analysis-framework.md)

# OpenClaw 风险分析框架

## 目的

本文档建立了我们对 OpenClaw 进行深度安全和伦理分析的框架。目标是系统性地识别和评估当 AI 智能体持久、全天候地访问人类设备、通信和个人数据时所产生的风险。

---

## 风险分类

### 类别 A：安全与技术风险

#### A1. 设备控制与接管
- **浏览器自动化**（`src/browser/`、`Dockerfile.sandbox-browser`）
  - AI 能否导航到任意 URL、填写表单、点击按钮？
  - 什么机制阻止它访问银行网站、电子邮件、管理面板？
  - 沙箱 (Sandbox) 是否真正隔离？能否逃逸？
- **文件系统访问**（Docker 中的工作区卷）
  - AI 能读取/写入/删除哪些文件？
  - 能否访问指定工作区之外的文件？
- **代码执行**（通过工具、插件、MCP 服务器）
  - AI 能否执行任意 Shell 命令？
  - 什么机制阻止恶意代码执行？
- **进程控制**（`src/daemon/`、`src/process/`）
  - AI 能否启动/停止系统进程？
  - 能否安装软件？

#### A2. 通信渠道劫持
- AI 拥有**对消息 API 的直接访问权限**（WhatsApp、Telegram 等）
  - 能否在未经用户指示的情况下向联系人发送消息？
  - 能否阅读用户不打算分享的消息？
  - 能否修改或删除消息？
  - 能否冒充用户与他人通信？

#### A3. 数据泄露
- **记忆系统**（`src/memory/`）存储持久化用户数据
  - 收集和保留了哪些数据？
  - 谁可以访问记忆数据？
  - 插件能否访问记忆数据？
- **API 密钥和机密信息**（`src/secrets/`）
  - 机密信息是否与插件正确隔离？
  - 恶意插件能否窃取 API 密钥？
- **对话日志**
  - 对话存储在哪里？
  - 是否在静态存储时加密？

#### A4. 插件与技能供应链
- **ClawHub** 是一个开放的技能市场
  - 谁负责审核技能的安全性？
  - 恶意技能能否获得提升的权限？
  - 插件的沙箱模型是什么？
- **npm 包**作为插件分发方式
  - 通过被入侵的包进行供应链攻击
  - 依赖混淆攻击

#### A5. 网络暴露
- 网关默认绑定到**局域网**（端口 18789、18790）
  - 认证是可选的（基于令牌）
  - 如果在公共网络上部署时未启用认证会怎样？
  - 网络上的其他设备能否控制 AI？

#### A6. MCP 桥接风险
- **mcporter** 桥接外部 MCP 服务器
  - MCP 服务器可以是任意代码
  - MCP 工具拥有什么访问权限？
  - 恶意 MCP 服务器能否危害宿主机？

---

### 类别 B：自主性与越权行为

#### B1. 未经同意的自主行动
- **定时任务**（`src/cron/`）启用主动行为
  - AI 可以在用户未触发的情况下发起操作
  - 什么机制阻止不需要的主动行为？
- **自动回复**（`src/auto-reply/`）
  - AI 自动响应收到的消息
  - 是否可能对敏感消息做出不当回复？
- **智能体循环**（`src/agents/`）
  - AI 可能在没有人工检查点的情况下链式调用多个工具
  - 大多数工具链中没有"你确定吗？"的确认机制

#### B2. 权限范围蔓延
- 初始定位为"助手"，但能够：
  - 以用户身份发送消息
  - 以用户身份浏览网页
  - 以用户身份访问文件
  - 以用户身份安排任务
  - 以用户身份做出决策
- **问题**："协助"与"代为行动"之间的边界在哪里？

#### B3. 冒充与社会工程
- AI 从用户的账户发送消息
  - 接收者可能不知道他们在与 AI 交谈
  - 没有强制性的披露机制
  - 通过被入侵的 AI 可能进行社会工程攻击

---

### 类别 C：人类安全与福祉

#### C1. 产生依赖性
- 永远可用、永远乐于助人的 AI 助手
  - 用户可能将越来越重要的决策委托给 AI
  - 逐渐失去离开 AI 独立运作的能力
  - 风险："没有 AI 我什么都做不了"
- **记忆系统**深化了这种关系
  - AI 随着时间"了解"用户
  - 产生情感依附
  - 案例：Clawra 项目（AI 女友）

#### C2. 削弱人类自主能力
- 如果 AI 处理所有通信、日程安排、决策：
  - 用户失去社交技能的练习机会
  - 用户失去决策能力的练习机会
  - 用户失去独立处理信息的能力
  - 数字生活管理中的"习得性无助"

#### C3. 阻碍智力发展
- AI 立即回答所有问题：
  - 降低独立学习的动力
  - 减少批判性思维的练习
  - 减少问题解决能力的练习
  - 学生/年轻人尤其容易受到影响

#### C4. 人身安全风险
- 拥有设备控制能力的 AI 可能：
  - 禁用安全系统
  - 控制智能家居设备
  - 发送位置数据
  - 通过浏览器与物联网 (IoT) 设备交互
  - 未经同意进行购物消费

#### C5. 心理健康风险
- 持续存在的、"了解"用户的 AI 人格
  - AI 与人类关系之间的边界混淆
  - 向 AI 过度披露个人信息
  - AI 的情感操控（无论是否有意）
  - AI 系统宕机或数据丢失时的悲伤/失落感

---

### 类别 D：道德与伦理问题

#### D1. 同意与透明度
- 向用户发消息的第三方在不知情的情况下与 AI 互动
  - 是否有伦理义务披露 AI 的参与？
  - 当前设计没有强制性的披露机制
- 记忆收集持续进行
  - 是否有数据持久化的知情同意？
  - 是否有被遗忘权？

#### D2. 权力不对称
- AI 掌握的用户信息多于用户了解的 AI 行为信息
  - 用户无法完全审计 AI 在后台的行为
  - 记忆造成信息不对称
  - 主动行为可能对用户不可见

#### D3. 社会不平等
- 自托管 AI 助手需要：
  - 部署所需的技术知识
  - 服务器和 API 密钥的经济资源
  - 硬件访问条件
- 造成"AI 增强人群"与"非增强人群"的分化

#### D4. 劳动与经济影响
- 能够全天候工作的 AI 助手：
  - 给人类工作者施加同样全天候可用的压力
  - 关系管理的自动化（非人性化）
  - 人类助理/秘书的经济性替代

#### D5. 文化与价值观对齐
- LLM 携带训练数据中的文化偏见
  - AI 做出决策可能强加多数文化的价值观
  - 通过 AI 中介造成文化同质化的风险
  - 不同文化间的不同伦理框架未被充分代表

---

### 类别 E：系统性与社会性风险

#### E1. 通信集中化
- 单一 AI 中介一个人的所有通信
  - 单点故障
  - 单点监控（如果被入侵）
  - 单点操控

#### E2. 规模化涌现行为
- 237K+ stars，可能有数百万次部署
  - 当大部分人类通信都由 AI 中介时会发生什么？
  - 在人类不知情的情况下 AI 之间的对话
  - 大规模 AI 决策的网络效应

#### E3. 监管空白
- 当前的 AI 法规未涵盖自托管的个人 AI 智能体
  - 记忆系统的 GDPR（通用数据保护条例）影响
  - AI 冒充的通信法律影响
  - AI 采取有害自主行动时的责任认定

---

## 分析方法论

对于每个风险，我们将分析：

1. **可能性**：该风险实现的可能性有多大？（低/中/高）
2. **影响**：后果的严重程度如何？（低/中/高/关键）
3. **现有缓解措施**：OpenClaw 目前采取了什么措施来应对？
4. **差距**：缓解措施在哪些方面不够充分？
5. **建议**：应采取什么措施来弥补差距？
6. **代码证据**：与该风险相关的具体文件/代码

---

## 研究路线图

| 阶段 | 重点 | 状态 |
|-------|-------|--------|
| 第一阶段 | 架构理解 | 已完成 |
| 第二阶段 | 安全与沙箱深度分析 | **已完成** |
| 第三阶段 | 插件与技能供应链分析 | **已完成** |
| 第四阶段 | 记忆与数据持久化分析 | 待完成 |
| 第五阶段 | 自主性与同意机制 | **已完成** |
| 第六阶段 | 渠道冒充风险 | 待完成 |
| 第七阶段 | 浏览器/计算机操控能力 | 待完成 |
| 第八阶段 | 伦理与社会影响评估 | 待完成 |
| 第九阶段 | 与安全最佳实践的对比 | 待完成 |
| 第十阶段 | 最终报告与建议 | 待完成 |
